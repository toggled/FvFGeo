{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R1eBTmF1O-zA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/anaconda3/envs/airfrans_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as nng\n",
    "import random\n",
    "\n",
    "\n",
    "import argparse, yaml, os, json, glob\n",
    "import torch\n",
    "import train, metrics\n",
    "from dataset import Dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJVMBW-mmHp-"
   },
   "source": [
    "**Redefining GUNet.py to use aSGCN convolutions**\n",
    "\n",
    "Original code: https://github.com/Extrality/AirfRANS/blob/main/models/GUNet.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "V4hEvbmW0w69",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time, json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as nng\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import metrics\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_nb_trainable_params(model):\n",
    "   '''\n",
    "   Return the number of trainable parameters\n",
    "   '''\n",
    "   model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "   return sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "def train(device, model, train_loader, optimizer, scheduler, criterion = 'MSE', reg = 1):\n",
    "    model.train()\n",
    "    avg_loss_per_var = torch.zeros(4, device = device)\n",
    "    avg_loss = 0\n",
    "    avg_loss_surf_var = torch.zeros(4, device = device)\n",
    "    avg_loss_vol_var = torch.zeros(4, device = device)\n",
    "    avg_loss_surf = 0\n",
    "    avg_loss_vol = 0\n",
    "    iter = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data_clone = data.clone()\n",
    "        data_clone = data_clone.to(device)          \n",
    "        optimizer.zero_grad()  \n",
    "        out = model(data_clone)\n",
    "        targets = data_clone.y\n",
    "\n",
    "        if criterion == 'MSE' or criterion == 'MSE_weighted':\n",
    "            criterion = nn.MSELoss(reduction = 'none')\n",
    "        elif criterion == 'MAE':\n",
    "            criterion = nn.L1Loss(reduction = 'none')\n",
    "        loss_per_var = criterion(out, targets).mean(dim = 0)\n",
    "        total_loss = loss_per_var.mean()\n",
    "        loss_surf_var = criterion(out[data_clone.surf, :], targets[data_clone.surf, :]).mean(dim = 0)\n",
    "        loss_vol_var = criterion(out[~data_clone.surf, :], targets[~data_clone.surf, :]).mean(dim = 0)\n",
    "        loss_surf = loss_surf_var.mean()\n",
    "        loss_vol = loss_vol_var.mean() \n",
    "        if criterion == 'MSE_weighted':            \n",
    "            (loss_vol + reg*loss_surf).backward()           \n",
    "        else:\n",
    "            total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        avg_loss_per_var += loss_per_var\n",
    "        avg_loss += total_loss\n",
    "        avg_loss_surf_var += loss_surf_var\n",
    "        avg_loss_vol_var += loss_vol_var\n",
    "        avg_loss_surf += loss_surf\n",
    "        avg_loss_vol += loss_vol \n",
    "        iter += 1\n",
    "\n",
    "    return avg_loss.cpu().data.numpy()/iter, avg_loss_per_var.cpu().data.numpy()/iter, avg_loss_surf_var.cpu().data.numpy()/iter, avg_loss_vol_var.cpu().data.numpy()/iter, \\\n",
    "            avg_loss_surf.cpu().data.numpy()/iter, avg_loss_vol.cpu().data.numpy()/iter\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(device, model, test_loader, criterion = 'MSE'):\n",
    "    model.eval()\n",
    "    avg_loss_per_var = np.zeros(4)\n",
    "    avg_loss = 0\n",
    "    avg_loss_surf_var = np.zeros(4)\n",
    "    avg_loss_vol_var = np.zeros(4)\n",
    "    avg_loss_surf = 0\n",
    "    avg_loss_vol = 0\n",
    "    iter = 0\n",
    "\n",
    "    for data in test_loader:        \n",
    "        data_clone = data.clone()\n",
    "        data_clone = data_clone.to(device)\n",
    "        out = model(data_clone)       \n",
    "\n",
    "        targets = data_clone.y\n",
    "        if criterion == 'MSE' or 'MSE_weighted':\n",
    "            criterion = nn.MSELoss(reduction = 'none')\n",
    "        elif criterion == 'MAE':\n",
    "            criterion = nn.L1Loss(reduction = 'none')\n",
    "\n",
    "        loss_per_var = criterion(out, targets).mean(dim = 0)\n",
    "        loss = loss_per_var.mean()\n",
    "        loss_surf_var = criterion(out[data_clone.surf, :], targets[data_clone.surf, :]).mean(dim = 0)\n",
    "        loss_vol_var = criterion(out[~data_clone.surf, :], targets[~data_clone.surf, :]).mean(dim = 0)\n",
    "        loss_surf = loss_surf_var.mean()\n",
    "        loss_vol = loss_vol_var.mean()  \n",
    "\n",
    "        avg_loss_per_var += loss_per_var.cpu().numpy()\n",
    "        avg_loss += loss.cpu().numpy()\n",
    "        avg_loss_surf_var += loss_surf_var.cpu().numpy()\n",
    "        avg_loss_vol_var += loss_vol_var.cpu().numpy()\n",
    "        avg_loss_surf += loss_surf.cpu().numpy()\n",
    "        avg_loss_vol += loss_vol.cpu().numpy()  \n",
    "        iter += 1\n",
    "    \n",
    "    return avg_loss/iter, avg_loss_per_var/iter, avg_loss_surf_var/iter, avg_loss_vol_var/iter, avg_loss_surf/iter, avg_loss_vol/iter\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def main(device, train_dataset, val_dataset, Net, hparams, path, criterion = 'MSE', reg = 1, val_iter = 10, name_mod = 'GraphSAGE', val_sample = False, use_saf = False,use_dsdf = False):\n",
    "    '''\n",
    "        Args:\n",
    "        device (str): device on which you want to do the computation.\n",
    "        train_dataset (list): list of the data in the training set.\n",
    "        val_dataset (list): list of the data in the validation set.\n",
    "        Net (class): network to train.\n",
    "        hparams (dict): hyper parameters of the network.\n",
    "        path (str): where to save the trained model and the figures.\n",
    "        criterion (str, optional): chose between 'MSE', 'MAE', and 'MSE_weigthed'. The latter is the volumetric MSE plus the surface MSE computed independently. Default: 'MSE'.\n",
    "        ref (float, optional): weigth for the surface loss when criterion is 'MSE_weighted'. Default: 1.\n",
    "        val_iter (int, optional): number of epochs between each validation step. Default: 10.\n",
    "    '''\n",
    "\n",
    "    model = Net.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = hparams['lr'])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr = hparams['lr'],\n",
    "            total_steps = (len(train_dataset) // hparams['batch_size'] + 1) * hparams['nb_epochs'],\n",
    "        )\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 1)\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss_surf_list = []\n",
    "    train_loss_vol_list = []\n",
    "    loss_surf_var_list = []\n",
    "    loss_vol_var_list = []\n",
    "    val_surf_list = []\n",
    "    val_vol_list = []\n",
    "    val_surf_var_list = []\n",
    "    val_vol_var_list = []\n",
    "\n",
    "    pbar_train = tqdm(range(hparams['nb_epochs']), position=0)\n",
    "    for epoch in pbar_train:        \n",
    "        train_dataset_sampled = []\n",
    "        for data in train_dataset:\n",
    "            data_sampled = data.clone()\n",
    "            idx = random.sample(range(data_sampled.x.size(0)), hparams['subsampling'])\n",
    "            idx = torch.tensor(idx)\n",
    "\n",
    "            data_sampled.pos = data_sampled.pos[idx]\n",
    "            data_sampled.x = data_sampled.x[idx]\n",
    "            data_sampled.y = data_sampled.y[idx]\n",
    "            data_sampled.surf = data_sampled.surf[idx]\n",
    "            if (use_saf):\n",
    "                data_sampled.saf = data_sampled.saf[idx]\n",
    "            if (use_dsdf):\n",
    "                # for i in range(data_sampled.dsdf.size(0)):\n",
    "                # data_sampled.dsdf[i] = data_sampled.dsdf[i][idx]\n",
    "                data_sampled.dsdf = data_sampled.dsdf[:,idx]\n",
    "            if name_mod != 'PointNet' and name_mod != 'MLP':\n",
    "                data_sampled.edge_index = nng.radius_graph(x = data_sampled.pos.to(device), r = hparams['r'], loop = True, max_num_neighbors = int(hparams['max_neighbors'])).cpu()\n",
    "\n",
    "                # if name_mod == 'GNO' or name_mod == 'MGNO':\n",
    "                #     x, edge_index = data_sampled.x, data_sampled.edge_index\n",
    "                #     x_i, x_j = x[edge_index[0], 0:2], x[edge_index[1], 0:2]\n",
    "                #     v_i, v_j = x[edge_index[0], 2:4], x[edge_index[1], 2:4]\n",
    "                #     p_i, p_j = x[edge_index[0], 4:5], x[edge_index[1], 4:5]\n",
    "                #     v_inf = torch.linalg.norm(v_i, dim = 1, keepdim = True)\n",
    "                #     sdf_i, sdf_j = x[edge_index[0], 5:6], x[edge_index[1], 5:6]\n",
    "                #     normal_i, normal_j = x[edge_index[0], 6:8], x[edge_index[1], 6:8]\n",
    "\n",
    "                #     data_sampled.edge_attr = torch.cat([x_i - x_j, v_i - v_j, p_i - p_j, sdf_i, sdf_j, v_inf, normal_i, normal_j], dim = 1)\n",
    "            \n",
    "            train_dataset_sampled.append(data_sampled)\n",
    "        train_loader = DataLoader(train_dataset_sampled, batch_size = hparams['batch_size'], shuffle = True)\n",
    "        # for data in train_dataset_sampled:\n",
    "        #     assert(data.x.size(0)==data.saf.size(0))\n",
    "        del(train_dataset_sampled)\n",
    "\n",
    "        _, _, loss_surf_var, loss_vol_var, loss_surf, loss_vol = train(device, model, train_loader, optimizer, lr_scheduler, criterion, reg = reg)\n",
    "        del(train_loader)\n",
    "        train_loss = loss_surf + loss_vol\n",
    "        train_loss_surf_list.append(loss_surf)\n",
    "        train_loss_vol_list.append(loss_vol)\n",
    "        loss_surf_var_list.append(loss_surf_var)\n",
    "        loss_vol_var_list.append(loss_vol_var)\n",
    "  \n",
    "        if val_iter is not None:\n",
    "            if epoch%val_iter == val_iter - 1 or epoch == 0:\n",
    "                if val_sample:\n",
    "                    val_surf_vars, val_vol_vars, val_surfs, val_vols = [], [], [], []\n",
    "                    for i in range(20):\n",
    "                        val_dataset_sampled = []\n",
    "                        for data in val_dataset:\n",
    "                            data_sampled = data.clone()\n",
    "                            idx = random.sample(range(data_sampled.x.size(0)), hparams['subsampling'])\n",
    "                            idx = torch.tensor(idx)\n",
    "\n",
    "                            data_sampled.pos = data_sampled.pos[idx]\n",
    "                            data_sampled.x = data_sampled.x[idx]\n",
    "                            data_sampled.y = data_sampled.y[idx]\n",
    "                            data_sampled.surf = data_sampled.surf[idx]\n",
    "                            if (use_saf):\n",
    "                                data_sampled.saf = data_sampled.saf[idx]\n",
    "                            if (use_dsdf):\n",
    "                                # for i in range(data_sampled.dsdf.size(0)):\n",
    "                                data_sampled.dsdf = data_sampled.dsdf[:,idx]\n",
    "                            if name_mod != 'PointNet' and name_mod != 'MLP':\n",
    "                                data_sampled.edge_index = nng.radius_graph(x = data_sampled.pos.to(device), r = hparams['r'], loop = True, max_num_neighbors = int(hparams['max_neighbors'])).cpu()\n",
    "\n",
    "                                # if name_mod == 'GNO' or name_mod == 'MGNO':\n",
    "                                #     x, edge_index = data_sampled.x, data_sampled.edge_index\n",
    "                                #     x_i, x_j = x[edge_index[0], 0:2], x[edge_index[1], 0:2]\n",
    "                                #     v_i, v_j = x[edge_index[0], 2:4], x[edge_index[1], 2:4]\n",
    "                                #     p_i, p_j = x[edge_index[0], 4:5], x[edge_index[1], 4:5]\n",
    "                                #     v_inf = torch.linalg.norm(v_i, dim = 1, keepdim = True)\n",
    "                                #     sdf_i, sdf_j = x[edge_index[0], 5:6], x[edge_index[1], 5:6]\n",
    "                                #     normal_i, normal_j = x[edge_index[0], 6:8], x[edge_index[1], 6:8]\n",
    "\n",
    "                                #     data_sampled.edge_attr = torch.cat([x_i - x_j, v_i - v_j, p_i - p_j, sdf_i, sdf_j, v_inf, normal_i, normal_j], dim = 1)\n",
    "                            \n",
    "                            val_dataset_sampled.append(data_sampled)\n",
    "                        val_loader = DataLoader(val_dataset_sampled, batch_size = 1, shuffle = True)\n",
    "                        del(val_dataset_sampled)\n",
    "\n",
    "                        _, _, val_surf_var, val_vol_var, val_surf, val_vol = test(device, model, val_loader, criterion)\n",
    "                        del(val_loader)\n",
    "                        val_surf_vars.append(val_surf_var)\n",
    "                        val_vol_vars.append(val_vol_var)\n",
    "                        val_surfs.append(val_surf)\n",
    "                        val_vols.append(val_vol)\n",
    "                    val_surf_var = np.array(val_surf_vars).mean(axis = 0)\n",
    "                    val_vol_var = np.array(val_vol_vars).mean(axis = 0)\n",
    "                    val_surf = np.array(val_surfs).mean(axis = 0)\n",
    "                    val_vol = np.array(val_vols).mean(axis = 0)\n",
    "                else:\n",
    "                    # if epoch == 0:\n",
    "                    #     for data in val_dataset:\n",
    "                    #         if name_mod != 'PointNet':\n",
    "                    #             data.edge_index = nng.radius_graph(x = data.pos.to(device), r = hparams['r'], loop = True, max_num_neighbors = int(hparams['max_neighbors'])).cpu()\n",
    "\n",
    "                    #             if name_mod == 'GNO' or name_mod == 'MGNO':\n",
    "                    #                 x, edge_index = data.x, data.edge_index\n",
    "                    #                 x_i, x_j = x[edge_index[0], 0:2], x[edge_index[1], 0:2]\n",
    "                    #                 v_i, v_j = x[edge_index[0], 2:4], x[edge_index[1], 2:4]\n",
    "                    #                 p_i, p_j = x[edge_index[0], 4:5], x[edge_index[1], 4:5]\n",
    "                    #                 v_inf = torch.linalg.norm(v_i, dim = 1, keepdim = True)\n",
    "                    #                 sdf_i, sdf_j = x[edge_index[0], 5:6], x[edge_index[1], 5:6]\n",
    "                    #                 normal_i, normal_j = x[edge_index[0], 6:8], x[edge_index[1], 6:8]\n",
    "\n",
    "                    #                 data.edge_attr = torch.cat([x_i - x_j, v_i - v_j, p_i - p_j, sdf_i, sdf_j, v_inf, normal_i, normal_j], dim = 1)\n",
    "                    #     val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False)\n",
    "                    _, _, val_surf_var, val_vol_var, val_surf, val_vol = test(device, model, val_loader, criterion)\n",
    "                val_loss = val_surf + val_vol\n",
    "                val_surf_list.append(val_surf)\n",
    "                val_vol_list.append(val_vol)\n",
    "                val_surf_var_list.append(val_surf_var)\n",
    "                val_vol_var_list.append(val_vol_var)\n",
    "                pbar_train.set_postfix(train_loss = train_loss, loss_surf = loss_surf, val_loss = val_loss, val_surf = val_surf)\n",
    "            else:\n",
    "                pbar_train.set_postfix(train_loss = train_loss, loss_surf = loss_surf, val_loss = val_loss, val_surf = val_surf)\n",
    "        else:\n",
    "            pbar_train.set_postfix(train_loss = train_loss, loss_surf = loss_surf)\n",
    "\n",
    "    loss_surf_var_list = np.array(loss_surf_var_list)\n",
    "    loss_vol_var_list = np.array(loss_vol_var_list)\n",
    "    val_surf_var_list = np.array(val_surf_var_list)\n",
    "    val_vol_var_list = np.array(val_vol_var_list)\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    params_model = get_nb_trainable_params(model).astype('float')\n",
    "    print('Number of parameters:', params_model)\n",
    "    print('Time elapsed: {0:.2f} seconds'.format(time_elapsed))\n",
    "    torch.save(model, path + 'model')\n",
    "\n",
    "    sns.set()\n",
    "    fig_train_surf, ax_train_surf = plt.subplots(figsize = (20, 5))\n",
    "    ax_train_surf.plot(train_loss_surf_list, label = 'Mean loss')\n",
    "    ax_train_surf.plot(loss_surf_var_list[:, 0], label = r'$v_x$ loss'); ax_train_surf.plot(loss_surf_var_list[:, 1], label = r'$v_y$ loss')\n",
    "    ax_train_surf.plot(loss_surf_var_list[:, 2], label = r'$p$ loss'); ax_train_surf.plot(loss_surf_var_list[:, 3], label = r'$\\nu_t$ loss')\n",
    "    ax_train_surf.set_xlabel('epochs')\n",
    "    ax_train_surf.set_yscale('log')\n",
    "    ax_train_surf.set_title('Train losses over the surface')\n",
    "    ax_train_surf.legend(loc = 'best')\n",
    "    fig_train_surf.savefig(path + 'train_loss_surf.png', dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "    fig_train_vol, ax_train_vol = plt.subplots(figsize = (20, 5))\n",
    "    ax_train_vol.plot(train_loss_vol_list, label = 'Mean loss')\n",
    "    ax_train_vol.plot(loss_vol_var_list[:, 0], label = r'$v_x$ loss'); ax_train_vol.plot(loss_vol_var_list[:, 1], label = r'$v_y$ loss')\n",
    "    ax_train_vol.plot(loss_vol_var_list[:, 2], label = r'$p$ loss'); ax_train_vol.plot(loss_vol_var_list[:, 3], label = r'$\\nu_t$ loss')\n",
    "    ax_train_vol.set_xlabel('epochs')\n",
    "    ax_train_vol.set_yscale('log')\n",
    "    ax_train_vol.set_title('Train losses over the volume')\n",
    "    ax_train_vol.legend(loc = 'best')\n",
    "    fig_train_vol.savefig(path + 'train_loss_vol.png', dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "    if val_iter is not None:\n",
    "        fig_val_surf, ax_val_surf = plt.subplots(figsize = (20, 5))\n",
    "        ax_val_surf.plot(val_surf_list, label = 'Mean loss')\n",
    "        ax_val_surf.plot(val_surf_var_list[:, 0], label = r'$v_x$ loss'); ax_val_surf.plot(val_surf_var_list[:, 1], label = r'$v_y$ loss')\n",
    "        ax_val_surf.plot(val_surf_var_list[:, 2], label = r'$p$ loss'); ax_val_surf.plot(val_surf_var_list[:, 3], label = r'$\\nu_t$ loss')\n",
    "        ax_val_surf.set_xlabel('epochs')\n",
    "        ax_val_surf.set_yscale('log')\n",
    "        ax_val_surf.set_title('Validation losses over the surface')\n",
    "        ax_val_surf.legend(loc = 'best')\n",
    "        fig_val_surf.savefig(path + 'val_loss_surf.png', dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "        fig_val_vol, ax_val_vol = plt.subplots(figsize = (20, 5))\n",
    "        ax_val_vol.plot(val_vol_list, label = 'Mean loss')\n",
    "        ax_val_vol.plot(val_vol_var_list[:, 0], label = r'$v_x$ loss'); ax_val_vol.plot(val_vol_var_list[:, 1], label = r'$v_y$ loss')\n",
    "        ax_val_vol.plot(val_vol_var_list[:, 2], label = r'$p$ loss'); ax_val_vol.plot(val_vol_var_list[:, 3], label = r'$\\nu_t$ loss')\n",
    "        ax_val_vol.set_xlabel('epochs')\n",
    "        ax_val_vol.set_yscale('log')\n",
    "        ax_val_vol.set_title('Validation losses over the volume')\n",
    "        ax_val_vol.legend(loc = 'best')\n",
    "        fig_val_vol.savefig(path + 'val_loss_vol.png', dpi = 150, bbox_inches = 'tight');\n",
    "        \n",
    "        if val_iter is not None:\n",
    "            with open(path + 'log.json', 'a') as f:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        'regression': 'Total',\n",
    "                        'loss': 'MSE',\n",
    "                        'nb_parameters': params_model,\n",
    "                        'time_elapsed': time_elapsed,\n",
    "                        'hparams': hparams,\n",
    "                        'train_loss_surf': train_loss_surf_list[-1],\n",
    "                        'train_loss_surf_var': loss_surf_var_list[-1],\n",
    "                        'train_loss_vol': train_loss_vol_list[-1],\n",
    "                        'train_loss_vol_var': loss_vol_var_list[-1],\n",
    "                        'val_loss_surf': val_surf_list[-1],\n",
    "                        'val_loss_surf_var': val_surf_var_list[-1],\n",
    "                        'val_loss_vol': val_vol_list[-1],\n",
    "                        'val_loss_vol_var': val_vol_var_list[-1],\n",
    "                    }, f, indent = 12, cls = NumpyEncoder\n",
    "                )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyvista as pv\n",
    "from reorganize import reorganize\n",
    "from math import *\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cell_sampling_2d(cell_points, cell_attr = None):\n",
    "    '''\n",
    "    Sample points in a two dimensional cell via parallelogram sampling and triangle interpolation via barycentric coordinates. The vertices have to be ordered in a certain way.\n",
    "\n",
    "    Args:\n",
    "        cell_points (array): Vertices of the 2 dimensional cells. Shape (N, 4) for N cells with 4 vertices.\n",
    "        cell_attr (array, optional): Features of the vertices of the 2 dimensional cells. Shape (N, 4, k) for N cells with 4 edges and k features. \n",
    "            If given shape (N, 4) it will resize it automatically in a (N, 4, 1) array. Default: ``None``\n",
    "    '''\n",
    "    # Sampling via triangulation of the cell and parallelogram sampling\n",
    "    v0, v1 = cell_points[:, 1] - cell_points[:, 0], cell_points[:, 3] - cell_points[:, 0]\n",
    "    v2, v3 = cell_points[:, 3] - cell_points[:, 2], cell_points[:, 1] - cell_points[:, 2]  \n",
    "    a0, a1 = np.abs(np.linalg.det(np.hstack([v0[:, :2], v1[:, :2]]).reshape(-1, 2, 2))), np.abs(np.linalg.det(np.hstack([v2[:, :2], v3[:, :2]]).reshape(-1, 2, 2)))\n",
    "    p = a0/(a0 + a1)\n",
    "    index_triangle = np.random.binomial(1, p)[:, None]\n",
    "    u = np.random.uniform(size = (len(p), 2))\n",
    "    sampled_point = index_triangle*(u[:, 0:1]*v0 + u[:, 1:2]*v1) + (1 - index_triangle)*(u[:, 0:1]*v2 + u[:, 1:2]*v3)\n",
    "    sampled_point_mirror = index_triangle*((1 - u[:, 0:1])*v0 + (1 - u[:, 1:2])*v1) + (1 - index_triangle)*((1 - u[:, 0:1])*v2 + (1 - u[:, 1:2])*v3)\n",
    "    reflex = (u.sum(axis = 1) > 1)\n",
    "    sampled_point[reflex] = sampled_point_mirror[reflex]\n",
    "\n",
    "    # Interpolation on a triangle via barycentric coordinates\n",
    "    if cell_attr is not None:\n",
    "        t0, t1, t2 = np.zeros_like(v0), index_triangle*v0 + (1 - index_triangle)*v2, index_triangle*v1 + (1 - index_triangle)*v3\n",
    "        w = (t1[:, 1] - t2[:, 1])*(t0[:, 0] - t2[:, 0]) + (t2[:, 0] - t1[:, 0])*(t0[:, 1] - t2[:, 1])\n",
    "        w0 = (t1[:, 1] - t2[:, 1])*(sampled_point[:, 0] - t2[:, 0]) + (t2[:, 0] - t1[:, 0])*(sampled_point[:, 1] - t2[:, 1])\n",
    "        w1 = (t2[:, 1] - t0[:, 1])*(sampled_point[:, 0] - t2[:, 0]) + (t0[:, 0] - t2[:, 0])*(sampled_point[:, 1] - t2[:, 1])\n",
    "        w0, w1 = w0/w, w1/w\n",
    "        w2 = 1 - w0 - w1\n",
    "        \n",
    "        if len(cell_attr.shape) == 2:\n",
    "            cell_attr = cell_attr[:, :, None]\n",
    "        attr0 = index_triangle*cell_attr[:, 0] + (1 - index_triangle)*cell_attr[:, 2]\n",
    "        attr1 = index_triangle*cell_attr[:, 1] + (1 - index_triangle)*cell_attr[:, 1]\n",
    "        attr2 = index_triangle*cell_attr[:, 3] + (1 - index_triangle)*cell_attr[:, 3]\n",
    "        sampled_attr = w0[:, None]*attr0 + w1[:, None]*attr1 + w2[:, None]*attr2\n",
    "\n",
    "    sampled_point += index_triangle*cell_points[:, 0] + (1 - index_triangle)*cell_points[:, 2]    \n",
    "\n",
    "    return np.hstack([sampled_point[:, :2], sampled_attr]) if cell_attr is not None else sampled_point[:, :2]\n",
    "\n",
    "def cell_sampling_1d(line_points, line_attr = None):\n",
    "    '''\n",
    "    Sample points in a one dimensional cell via linear sampling and interpolation.\n",
    "\n",
    "    Args:\n",
    "        line_points (array): Edges of the 1 dimensional cells. Shape (N, 2) for N cells with 2 edges.\n",
    "        line_attr (array, optional): Features of the edges of the 1 dimensional cells. Shape (N, 2, k) for N cells with 2 edges and k features.\n",
    "            If given shape (N, 2) it will resize it automatically in a (N, 2, 1) array. Default: ``None``\n",
    "    '''\n",
    "    # Linear sampling\n",
    "    u = np.random.uniform(size = (len(line_points), 1))\n",
    "    sampled_point = u*line_points[:, 0] + (1 - u)*line_points[:, 1]\n",
    "\n",
    "    # Linear interpolation\n",
    "    if line_attr is not None:   \n",
    "        if len(line_attr.shape) == 2:\n",
    "            line_attr = line_attr[:, :, None]\n",
    "        sampled_attr = u*line_attr[:, 0] + (1 - u)*line_attr[:, 1]\n",
    "\n",
    "    return np.hstack([sampled_point[:, :2], sampled_attr]) if line_attr is not None else sampled_point[:, :2]\n",
    "\n",
    "def computeSAF(pos,surf_bool,aerofoil):\n",
    "    # def angle_trunc(a):\n",
    "    #     while (a < 0.0).any():\n",
    "    #         a[a<0.0] += (pi * 2)\n",
    "    #     return a\n",
    "    def getAngleBetweenPoints(xy_orig, xy_landmark):\n",
    "        \"\"\" Returns angle in radians \"\"\"\n",
    "        x_landmark,y_landmark = xy_landmark[:,0],xy_landmark[:,1]\n",
    "        x_orig,y_orig = xy_orig[:,0],xy_orig[:,1]\n",
    "        deltaY = y_landmark - y_orig\n",
    "        deltaX = x_landmark - x_orig\n",
    "        # return angle_trunc(np.arctan2(deltaY, deltaX)) # Do not use this\n",
    "        return np.arctan2(deltaY,deltaX)\n",
    "\n",
    "    def get_closest_points(internalcells, boundarycells):\n",
    "        dist = cdist(boundarycells,internalcells)\n",
    "        closests = np.argmin(dist,axis=0)\n",
    "        return closests, boundarycells[closests]\n",
    "        \n",
    "    # closest_point= aerofoil.find_closest_cell(pos)\n",
    "    closest_point,closest_points_xyz = get_closest_points(pos,pos[surf_bool])\n",
    "    # print(closest_point)\n",
    "    # print('shape: ',closest_point.shape,' aerofoil points shape: ',aerofoil.points.shape)\n",
    "    # closest_points_xyz = aerofoil.points[closest_point]\n",
    "    # print('closest_points_xyz: ',closest_points_xyz.shape)\n",
    "    saf_tmp = getAngleBetweenPoints(closest_points_xyz[:,:2],pos[:,:2]) # Angle in radian\n",
    "    saf_tmp = (saf_tmp)/1.8128\n",
    "    return saf_tmp\n",
    "\n",
    "def GcomputeSAF(pos,surf_bool): # Assume pos, surf_bool are torch tensors\n",
    "    # def angle_trunc(a):\n",
    "    #     while (a < 0.0).any():\n",
    "    #         a[a<0.0] += (pi * 2)\n",
    "    #     return a\n",
    "    def getAngleBetweenPoints(xy_orig, xy_landmark):\n",
    "        \"\"\" Returns angle in radians \"\"\"\n",
    "        x_landmark,y_landmark = xy_landmark[:,0],xy_landmark[:,1]\n",
    "        x_orig,y_orig = xy_orig[:,0],xy_orig[:,1]\n",
    "        deltaY = y_landmark - y_orig\n",
    "        deltaX = x_landmark - x_orig\n",
    "        return torch.atan2(deltaY,deltaX)\n",
    "\n",
    "    def get_closest_points(internalcells, boundarycells):\n",
    "        dist = torch.cdist(boundarycells,internalcells,p=2)\n",
    "        # print('dist size: ',dist.size())\n",
    "        closests = torch.argmin(dist,dim=0)\n",
    "        return closests, boundarycells[closests]\n",
    "        \n",
    "    # closest_point= aerofoil.find_closest_cell(pos)\n",
    "    closest_point,closest_points_xyz = get_closest_points(pos,pos[surf_bool])\n",
    "    # print(closest_point)\n",
    "    # print('shape: ',closest_point.shape,' aerofoil points shape: ',aerofoil.points.shape)\n",
    "    # closest_points_xyz = aerofoil.points[closest_point]\n",
    "    # print('closest_points_xyz: ',closest_points_xyz.shape)\n",
    "    saf_tmp = getAngleBetweenPoints(closest_points_xyz[:,:2],pos[:,:2]) # Angle in radian\n",
    "    # saf_tmp = (saf_tmp)/1.8128 # normalization => subtract mean([-pi,pi]), divide by std([-pi,pi])\n",
    "    return saf_tmp\n",
    "\n",
    "def getDSDF(pos, bd, theta_rot, theta_seg, inf=5):\n",
    "    '''get dSDF representation of geometry given rotation angle, segment angle.'''\n",
    "    # print('----- DSDF ---------')\n",
    "    # print('type(pos) & shape(pos): ', type(pos), pos.size())\n",
    "    # print('type(bd) & shape(bd): ',type(bd), bd.size())\n",
    "    def order_clockwise(bd_xy):\n",
    "        c = torch.mean(bd_xy,dim=0)\n",
    "        h = bd_xy - c\n",
    "        theta = torch.atan2(h[:,1],h[:,0]) #size MxN\n",
    "        return torch.flipud(bd_xy[theta.sort()[1]])\n",
    "    def sameSide(j,i):\n",
    "        '''Check if boundary points j (Mx2) and all points i (Nx2)\n",
    "        fall on the same side of the geometry (bool NxM)'''\n",
    "        # c = torch.tensor([[0.5,0]]).to(j.device) #airfoil internal point\n",
    "        c = torch.mean(j,dim=0).unsqueeze(0)\n",
    "        j = order_clockwise(j); j1 = torch.cat([j[1:,:],j[0:1,:]])\n",
    "        indi, indj = torch.meshgrid(torch.arange(i.size(0)), \\\n",
    "                                torch.arange(j.size(0)), indexing='ij')\n",
    "        p0 = i[indi]; p1 = j[indj]; p2 = j1[indj]\n",
    "        dir_i = (p1[:,:,1]-p0[:,:,1])*(p2[:,:,0]-p1[:,:,0])\n",
    "        dir_i -= (p2[:,:,1]-p1[:,:,1])*(p1[:,:,0]-p0[:,:,0])\n",
    "        dir_c = (j[:,1]-c[:,1])*(j1[:,0]-j[:,0])\n",
    "        dir_c -= (j1[:,1]-j[:,1])*(j[:,0]-c[:,0])\n",
    "        side = ((dir_i>0)==(dir_c<=0)).t() #bool size NxM\n",
    "        return j,side\n",
    "\n",
    "    def PDF(mean,x):\n",
    "        '''chosen PDF to used to weigh points'''\n",
    "        return torch.ones(x.size()).to(d.device) #if just uniform distribution\n",
    "        # grad = 1/theta_seg**2;\n",
    "        # w0 = (x-(mean-theta_seg/2))/(theta_seg**2)\n",
    "        # w1 = (-x+(mean+theta_seg/2))/(theta_seg**2) #linear\n",
    "        # return w0*(x<=mean)+w1*(x>mean) #if linear distribution\n",
    "    \n",
    "    def intPDF(mean,mRange):\n",
    "        '''integral of chosen PDF used to weigh points'''\n",
    "        return (mRange[1]-mRange[0]) #for uniform distribution\n",
    "        # dw = PDF(mean,mRange); grad = 1/theta_seg**2;\n",
    "        # w0 = 0.5*dw*(mRange-(mean-theta_seg/2))\n",
    "        # w1 = 1 - (0.5*dw*((mean+theta_seg/2)-mRange)) #linear\n",
    "        # w = (w0*(mRange<=mean)+w1*(mRange>mean))*(dw>=0)\n",
    "        # return w[1]-w[0] #if linear distribution\n",
    "\n",
    "    dSDF = []; j = pos[bd]; j, side = sameSide(j,pos)\n",
    "    #Compute dSDF for ever segment centre theta_cen\n",
    "    for theta_cen in torch.arange(0,2*torch.pi,theta_rot):\n",
    "        #Compute segment range theta_ran\n",
    "        theta_ran = torch.tensor([theta_cen-(theta_seg/2), theta_cen+(theta_seg/2)])\n",
    "        dSDF_tCen = []\n",
    "        \n",
    "        #Compute dSDF_i for every node i in V\n",
    "        h = j[:,0:1] - pos.t()[0:1,:] #size MxN\n",
    "        k = j[:,1:] - pos.t()[1:,:] #size MxN\n",
    "        theta_ij = torch.atan2(k,h) #size MxN\n",
    "        theta_ij = theta_ij%(2*torch.pi) if theta_ran[0]>=0 else theta_ij\n",
    "        \n",
    "        #Compute distance from point i to every bd point within segment range AND same side\n",
    "        ind = (theta_ran[0]<=theta_ij)*(theta_ij<=theta_ran[1])*side\n",
    "        d = torch.sqrt(k*k+h*h); d[d>inf]=inf #size MxN\n",
    "        w = PDF(theta_cen,theta_ij) #size MxN\n",
    "        dSDF_i = torch.sum((d*w)*ind,dim=0) #dSDF_i = torch.sum((d*ind),dim=0)\n",
    "        w = torch.sum((w*ind),dim=0) #w=torch.sum(ind,dim=0) #sum of discrete weights\n",
    "        \n",
    "        #minimum angle range from point i to point j\n",
    "        theta_minR = torch.zeros(2,theta_ij.size(1)).to(dSDF_i.device)\n",
    "        ind = (theta_ran[0]<=theta_ij)*(theta_ij<=theta_ran[1]) # in segment range (but any side)\n",
    "        theta_ij[~ind] = 2*torch.pi; theta_minR[0] = torch.min(theta_ij,dim=0)[0]\n",
    "        theta_ij[~ind] = -2*torch.pi; theta_minR[1] = torch.max(theta_ij,dim=0)[0]\n",
    "        \n",
    "        #compute weight of minimum angle segment w_tMin\n",
    "        w_tMin = intPDF(theta_cen,theta_minR)/intPDF(theta_cen,theta_ran)\n",
    "        w_tMin[w_tMin<0] = 0\n",
    "        dSDF_i = torch.nan_to_num(dSDF_i/w,nan=0)\n",
    "        dSDF_tCen = w_tMin*dSDF_i + (1-w_tMin)*inf\n",
    "        dSDF.append(dSDF_tCen.clone())\n",
    "    dSDF = torch.stack(dSDF)\n",
    "    return dSDF\n",
    "  \n",
    "\n",
    "def Dataset(set, norm = False, coef_norm = None, crop = None, sample = None, n_boot = int(5e5), surf_ratio = .1, \\\n",
    "    use_saf=False,use_dsdf=False, manifest_dsdf=None,manifest_saf=None):\n",
    "    '''\n",
    "    Create a list of simulation to input in a PyTorch Geometric DataLoader. Simulation are transformed by keeping vertices of the CFD mesh or \n",
    "    by sampling (uniformly or via the mesh density) points in the simulation cells.\n",
    "\n",
    "    Args:\n",
    "        set (list): List of geometry names to include in the dataset.\n",
    "        norm (bool, optional): If norm is set to ``True``, the mean and the standard deviation of the dataset will be computed and returned. \n",
    "            Moreover, the dataset will be normalized by these quantities. Ignored when ``coef_norm`` is not None. Default: ``False``\n",
    "        coef_norm (tuple, optional): This has to be a tuple of the form (mean input, std input, mean output, std ouput) if not None. \n",
    "            The dataset generated will be normalized by those quantites. Default: ``None``\n",
    "        crop (list, optional): List of the vertices of the rectangular [xmin, xmax, ymin, ymax] box to crop simulations. Default: ``None``\n",
    "        sample (string, optional): Type of sampling. If ``None``, no sampling strategy is applied and the nodes of the CFD mesh are returned.\n",
    "            If ``uniform`` or ``mesh`` is chosen, uniform or mesh density sampling is applied on the domain. Default: ``None``\n",
    "        n_boot (int, optional): Used only if sample is not None, gives the size of the sampling for each simulation. Defaul: ``int(5e5)``\n",
    "        surf_ratio (float, optional): Used only if sample is not None, gives the ratio of point over the airfoil to sample with respect to point\n",
    "            in the volume. Default: ``0.1``\n",
    "    '''\n",
    "    if norm and coef_norm is not None:\n",
    "        raise ValueError('If coef_norm is not None and norm is True, the normalization will be done via coef_norm')\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for k, s in enumerate(tqdm(set)):\n",
    "        # Get the 3D mesh, add the signed distance function and slice it to return in 2D\n",
    "        internal = pv.read('Dataset/' + s + '/' + s + '_internal.vtu')\n",
    "        aerofoil = pv.read('Dataset/' + s + '/' + s + '_aerofoil.vtp')\n",
    "        internal = internal.compute_cell_sizes(length = False, volume = False)\n",
    "        # Cropping if needed, crinkle is True.\n",
    "        if crop is not None:\n",
    "            bounds = (crop[0], crop[1], crop[2], crop[3], 0, 1)\n",
    "            internal = internal.clip_box(bounds = bounds, invert = False, crinkle = True)\n",
    "\n",
    "        # If sampling strategy is chosen, it will sample points in the cells of the simulation instead of directly taking the nodes of the mesh.\n",
    "        if sample is not None:\n",
    "            # Sample on a new point cloud\n",
    "            if sample == 'uniform': # Uniform sampling strategy\n",
    "                p = internal.cell_data['Area']/internal.cell_data['Area'].sum()\n",
    "                sampled_cell_indices = np.random.choice(internal.n_cells, size = n_boot, p = p)\n",
    "                surf_p = aerofoil.cell_data['Length']/aerofoil.cell_data['Length'].sum()\n",
    "                sampled_line_indices = np.random.choice(aerofoil.n_cells, size = int(n_boot*surf_ratio), p = surf_p)\n",
    "            elif sample == 'mesh': # Sample via mesh density\n",
    "                sampled_cell_indices = np.random.choice(internal.n_cells, size = n_boot)\n",
    "                sampled_line_indices = np.random.choice(aerofoil.n_cells, size = int(n_boot*surf_ratio))\n",
    "\n",
    "            cell_dict = internal.cells.reshape(-1, 5)[sampled_cell_indices, 1:]\n",
    "            cell_points = internal.points[cell_dict]            \n",
    "            line_dict = aerofoil.lines.reshape(-1, 3)[sampled_line_indices, 1:]\n",
    "            line_points = aerofoil.points[line_dict]\n",
    "\n",
    "            # Geometry information\n",
    "            geom = -internal.point_data['implicit_distance'][cell_dict, None] # Signed distance function\n",
    "            Uinf, alpha = float(s.split('_')[2]), float(s.split('_')[3])*np.pi/180\n",
    "            # u = (np.array([np.cos(alpha), np.sin(alpha)])*Uinf).reshape(1, 2)*(internal.point_data['U'][cell_dict, :1] != 0)\n",
    "            u = (np.array([np.cos(alpha), np.sin(alpha)])*Uinf).reshape(1, 2)*np.ones_like(internal.point_data['U'][cell_dict, :1])\n",
    "            normal = np.zeros_like(u)\n",
    "\n",
    "            surf_geom = np.zeros_like(aerofoil.point_data['U'][line_dict, :1])\n",
    "            # surf_u = np.zeros_like(aerofoil.point_data['U'][line_dict, :2])\n",
    "            surf_u = (np.array([np.cos(alpha), np.sin(alpha)])*Uinf).reshape(1, 2)*np.ones_like(aerofoil.point_data['U'][line_dict, :1])\n",
    "            surf_normal = -aerofoil.point_data['Normals'][line_dict, :2]\n",
    "\n",
    "            attr = np.concatenate([u, geom, normal, internal.point_data['U'][cell_dict, :2], \n",
    "                internal.point_data['p'][cell_dict, None], internal.point_data['nut'][cell_dict, None]], axis = -1)\n",
    "            surf_attr = np.concatenate([surf_u, surf_geom, surf_normal, aerofoil.point_data['U'][line_dict, :2], \n",
    "                aerofoil.point_data['p'][line_dict, None], aerofoil.point_data['nut'][line_dict, None]], axis = -1)\n",
    "            sampled_points = cell_sampling_2d(cell_points, attr)\n",
    "            surf_sampled_points = cell_sampling_1d(line_points, surf_attr)\n",
    "\n",
    "            # Define the inputs and the targets\n",
    "            pos = sampled_points[:, :2]\n",
    "            init = sampled_points[:, :7]\n",
    "            target = sampled_points[:, 7:]\n",
    "            surf_pos = surf_sampled_points[:, :2]\n",
    "            surf_init = surf_sampled_points[:, :7]\n",
    "            surf_target = surf_sampled_points[:, 7:]\n",
    "\n",
    "            # if cell_centers:\n",
    "            #     centers = internal.ptc().cell_centers()\n",
    "            #     surf_centers = aerofoil.cell_centers()\n",
    "\n",
    "            #     geom = -centers.cell_data['implicit_distance'][:, None] # Signed distance function\n",
    "            #     Uinf, alpha = float(s.split('_')[2]), float(s.split('_')[3])*np.pi/180\n",
    "            #     u = (np.array([np.cos(alpha), np.sin(alpha)])*Uinf).reshape(1, 2)*np.ones_like(internal.cell_data['U'][:, :1])\n",
    "            #     normal = np.zeros_like(u)\n",
    "\n",
    "            #     surf_geom = np.zeros_like(surf_centers.cell_data['U'][:, :1])\n",
    "            #     # surf_u = np.zeros_like(surf_centers.cell_data['U'][:, :2])\n",
    "            #     surf_u = (np.array([np.cos(alpha), np.sin(alpha)])*Uinf).reshape(1, 2)*np.ones_like(surf_centers.cell_data['U'][:, :1])\n",
    "            #     surf_normal = -aerofoil.cell_data['Normals'][:, :2]\n",
    "\n",
    "            #     attr = np.concatenate([u, geom, normal,\n",
    "            #         internal.cell_data['U'][:, :2], internal.cell_data['p'][:, None], internal.cell_data['nut'][:, None]], axis = -1)\n",
    "            #     surf_attr = np.concatenate([surf_u, surf_geom, surf_normal,\n",
    "            #         aerofoil.cell_data['U'][:, :2], aerofoil.cell_data['p'][:, None], aerofoil.cell_data['nut'][:, None]], axis = -1)\n",
    "\n",
    "            #     bool_centers = np.concatenate([np.ones_like(centers.points[:, 0]), np.zeros_like(pos[:, 0])], axis = 0)\n",
    "            #     surf_bool_centers = np.concatenate([np.ones_like(surf_centers.points[:, 0]), np.zeros_like(surf_pos[:, 0])], axis = 0)\n",
    "            #     pos = np.concatenate([centers.points[:, :2], pos], axis = 0)\n",
    "            #     init = np.concatenate([np.concatenate([centers.points[:, :2], attr[:, :6]], axis = 1), init], axis = 0)\n",
    "            #     target = np.concatenate([attr[:, 6:], target], axis = 0)\n",
    "            #     surf_pos = np.concatenate([surf_centers.points[:, :2], surf_pos], axis = 0)\n",
    "            #     surf_init = np.concatenate([np.concatenate([surf_centers.points[:, :2], surf_attr[:, :6]], axis = 1), surf_init], axis = 0)\n",
    "            #     surf_target = np.concatenate([surf_attr[:, 6:], surf_target], axis = 0)\n",
    "\n",
    "            #     centers = torch.cat([torch.tensor(bool_centers), torch.tensor(surf_bool_centers)], dim = 0)\n",
    "\n",
    "            # Put everything in tensor\n",
    "            surf = torch.cat([torch.zeros(len(pos)), torch.ones(len(surf_pos))], dim = 0)\n",
    "            pos = torch.cat([torch.tensor(pos, dtype = torch.float), torch.tensor(surf_pos, dtype = torch.float)], dim = 0) \n",
    "            x = torch.cat([torch.tensor(init, dtype = torch.float), torch.tensor(surf_init, dtype = torch.float)], dim = 0)\n",
    "            y = torch.cat([torch.tensor(target, dtype = torch.float), torch.tensor(surf_target, dtype = torch.float)], dim = 0)            \n",
    "\n",
    "        else: # Keep the mesh nodes\n",
    "            surf_bool = (internal.point_data['U'][:, 0] == 0)\n",
    "\n",
    "            # print('surf_bool: ',surf_bool)\n",
    "            geom = -internal.point_data['implicit_distance'][:, None] # Signed distance function\n",
    "            # print('sdf: ',geom)\n",
    "            Uinf, alpha = float(s.split('_')[2]), float(s.split('_')[3])*np.pi/180\n",
    "            # print('Uinf: ',Uinf)\n",
    "            # print('alpha: ',alpha)\n",
    "            # u = (np.array([np.cos(alpha), np.sin(alpha)])*Uinf).reshape(1, 2)*(internal.point_data['U'][:, :1] != 0)\n",
    "            u = (np.array([np.cos(alpha), np.sin(alpha)])*Uinf).reshape(1, 2)*np.ones_like(internal.point_data['U'][:, :1])\n",
    "            # print('u: ',u)\n",
    "            normal = np.zeros_like(u)\n",
    "            normal[surf_bool] = reorganize(aerofoil.points[:, :2], internal.points[surf_bool, :2], -aerofoil.point_data['Normals'][:, :2])\n",
    "            # print('internal.point_data keys: ',internal.point_data.keys())\n",
    "\n",
    "            # print('u , geom, normal, internal.point_data[u] [p] [nut] shapes:')\n",
    "            # print(u.shape)\n",
    "            # print(geom.shape)\n",
    "            # print(normal.shape)\n",
    "            # print(internal.point_data['U'].shape)\n",
    "            # print(internal.point_data['p'].shape)\n",
    "            # print(internal.point_data['nut'].shape)\n",
    "            attr = np.concatenate([u, geom, normal,\n",
    "                internal.point_data['U'][:, :2], internal.point_data['p'][:, None], internal.point_data['nut'][:, None]], axis = -1)\n",
    "\n",
    "            # print('attr shape: ',attr.shape)\n",
    "            # print('first 5 columns are x')\n",
    "            # print(attr.shape[1]-5,' columns are y')\n",
    "            pos = internal.points[:, :2] # cx,cy\n",
    "            init = np.concatenate([pos, attr[:, :5]], axis = 1) # cx,cy, ux,uy,sdf,nx,ny\n",
    "            target = attr[:, 5:] # U\n",
    "            # saf = computeSAF(internal.points[:,:3],surf_bool,aerofoil)\n",
    "            # Plotting SDF\n",
    "            # if k==0:\n",
    "            #     F = ( (pos[:,0]<2) & (pos[:,0]>-1)  & (pos[:,1]>-0.5) & (pos[:,1]<0.5))\n",
    "            #     xyF = pos[F]\n",
    "            #     sdfF = geom[F]\n",
    "            #     plt.scatter(xyF[:,0],xyF[:,1],c=sdfF,s=1,cmap = 'viridis'); plt.colorbar(); plt.savefig(s+'_SDF.png'); plt.clf()\n",
    "            # # Plotting SAF\n",
    "            # if k<4:\n",
    "            #     F = ( (pos[:,0]<2) & (pos[:,0]>-1)  & (pos[:,1]>-0.5) & (pos[:,1]<0.5))\n",
    "            #     xyF = pos[F]\n",
    "            #     safF = saf[F]\n",
    "            #     plt.scatter(xyF[:,0],xyF[:,1],c=safF,s=1,cmap = 'viridis'); plt.colorbar(); plt.savefig(s+'_SAF.png'); plt.clf()\n",
    "            # Put everything in tensor\n",
    "            surf = torch.tensor(surf_bool)\n",
    "            pos = torch.tensor(pos, dtype = torch.float)\n",
    "            x = torch.tensor(init, dtype = torch.float)\n",
    "            y = torch.tensor(target, dtype = torch.float) # ux,uy,sdf,nx,ny\n",
    "            #print('target y :',y.shape)\n",
    "\n",
    "        if norm and coef_norm is None:\n",
    "            if k == 0:\n",
    "                old_length = init.shape[0]\n",
    "                mean_in = init.mean(axis = 0, dtype = np.double)\n",
    "                mean_out = target.mean(axis = 0, dtype = np.double)\n",
    "            else:\n",
    "                new_length = old_length + init.shape[0]\n",
    "                mean_in += (init.sum(axis = 0, dtype = np.double) - init.shape[0]*mean_in)/new_length\n",
    "                mean_out += (target.sum(axis = 0, dtype = np.double) - init.shape[0]*mean_out)/new_length\n",
    "                old_length = new_length \n",
    "\n",
    "        # Graph definition\n",
    "        # if cell_centers:\n",
    "        #     data = Data(pos = pos, x = x, y = y, surf = surf.bool(), centers = centers.bool())\n",
    "        # else:\n",
    "        #     data = Data(pos = pos, x = x, y = y, surf = surf.bool())\n",
    "        if use_saf:\n",
    "            saf = torch.load(manifest_saf[s])\n",
    "        else:\n",
    "            saf = None \n",
    "        if (use_dsdf):\n",
    "            dsdf = torch.load(manifest_dsdf[s])\n",
    "        else:\n",
    "            dsdf = None\n",
    "        data = Data(pos = pos, x = x, y = y, surf = surf.bool(),saf = saf,dsdf=dsdf)\n",
    "        dataset.append(data)\n",
    "\n",
    "    if norm and coef_norm is None:\n",
    "        # Compute normalization\n",
    "        mean_in = mean_in.astype(np.single)\n",
    "        mean_out = mean_out.astype(np.single)\n",
    "        # Umean = np.linalg.norm(data.x[:, 2:4], axis = 1).mean()     \n",
    "        for k, data in enumerate(dataset):\n",
    "            # data.x = data.x/torch.tensor([6, 6, Umean, Umean, 6, 1, 1], dtype = torch.float)\n",
    "            # data.y = data.y/torch.tensor([Umean, Umean, .5*Umean**2, Umean], dtype = torch.float)\n",
    "\n",
    "            if k == 0:\n",
    "                old_length = data.x.numpy().shape[0]\n",
    "                std_in = ((data.x.numpy() - mean_in)**2).sum(axis = 0, dtype = np.double)/old_length\n",
    "                std_out = ((data.y.numpy() - mean_out)**2).sum(axis = 0, dtype = np.double)/old_length\n",
    "            else:\n",
    "                new_length = old_length + data.x.numpy().shape[0]\n",
    "                std_in += (((data.x.numpy() - mean_in)**2).sum(axis = 0, dtype = np.double) - data.x.numpy().shape[0]*std_in)/new_length\n",
    "                std_out += (((data.y.numpy() - mean_out)**2).sum(axis = 0, dtype = np.double) - data.x.numpy().shape[0]*std_out)/new_length\n",
    "                old_length = new_length\n",
    "        \n",
    "        std_in = np.sqrt(std_in).astype(np.single)\n",
    "        std_out = np.sqrt(std_out).astype(np.single)\n",
    "\n",
    "        # Normalize\n",
    "        for data in dataset:\n",
    "            data.x = (data.x - mean_in)/(std_in + 1e-8)\n",
    "            data.y = (data.y - mean_out)/(std_out + 1e-8)\n",
    "\n",
    "        coef_norm = (mean_in, std_in, mean_out, std_out)     \n",
    "        dataset = (dataset, coef_norm)   \n",
    "    \n",
    "    elif coef_norm is not None:\n",
    "        # Normalize\n",
    "        for data in dataset:\n",
    "            # data.x = data.x/torch.tensor([6, 6, coef_norm[-1], coef_norm[-1], 6, 1, 1], dtype = torch.float)\n",
    "            # data.y = data.y/torch.tensor([coef_norm[-1], coef_norm[-1], .5*coef_norm[-1]**2, coef_norm[-1]], dtype = torch.float)\n",
    "            data.x = (data.x - coef_norm[0])/(coef_norm[1] + 1e-8)\n",
    "            data.y = (data.y - coef_norm[2])/(coef_norm[3] + 1e-8)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2149973580.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    GraphSAGE:\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "GraphSAGE:\n",
    "  encoder: [7, 64, 64, 8]\n",
    "  decoder: [8, 64, 64, 4]\n",
    "\n",
    "  nb_hidden_layers: 3\n",
    "  size_hidden_layers: 64\n",
    "  batch_size: 1\n",
    "  nb_epochs: 398\n",
    "  lr: 0.001\n",
    "  max_neighbors: 64\n",
    "  bn_bool: True\n",
    "  subsampling: 32000\n",
    "  r: 0.05\n",
    "\n",
    "PointNet:\n",
    "  encoder: [7, 64, 64, 8]\n",
    "  decoder: [8, 64, 64, 4]\n",
    "\n",
    "  base_nb: 8\n",
    "  batch_size: 1\n",
    "  nb_epochs: 398\n",
    "  lr: 0.001\n",
    "  subsampling: 32000\n",
    "\n",
    "MLP:\n",
    "  encoder: [7, 64, 64, 8]\n",
    "  decoder: [8, 64, 64, 4]\n",
    "\n",
    "  nb_hidden_layers: 3\n",
    "  size_hidden_layers: 64\n",
    "  batch_size: 1\n",
    "  nb_epochs: 398\n",
    "  lr: 0.001\n",
    "  bn_bool: True\n",
    "  subsampling: 32000\n",
    "\n",
    "GUNet:\n",
    "  #encoder: [7, 64, 64, 8] #EDITED\n",
    "  encoder: [16, 64, 64, 8] #EDITED####\n",
    "  decoder: [8, 64, 64, 4]\n",
    "\n",
    "  layer: 'SAGE'\n",
    "  pool: 'random'\n",
    "  nb_scale: 5\n",
    "  pool_ratio: [.5, .5, .5, .5]\n",
    "  list_r: [.05, .2, .5, 1, 10]\n",
    "  size_hidden_layers: 8\n",
    "  batchnorm: True\n",
    "  res: False\n",
    "\n",
    "  batch_size: 1\n",
    "  nb_epochs: 398\n",
    "  lr: 0.001\n",
    "  max_neighbors: 64\n",
    "  subsampling: 32000\n",
    "  r: 0.05\n",
    "\n",
    "GUNetFVGCN:\n",
    "  #encoder: [7, 64, 64, 8] #EDITED\n",
    "  encoder: [16, 64, 64, 8] #EDITED####\n",
    "  decoder: [8, 64, 64, 4]\n",
    "\n",
    "  layer: 'SAGE'\n",
    "  pool: 'random'\n",
    "  nb_scale: 5\n",
    "  pool_ratio: [.5, .5, .5, .5]\n",
    "  list_r: [.05, .2, .5, 1, 10]\n",
    "  size_hidden_layers: 8\n",
    "  batchnorm: True\n",
    "  res: False\n",
    "\n",
    "  batch_size: 1\n",
    "  nb_epochs: 398\n",
    "  lr: 0.001\n",
    "  max_neighbors: 64\n",
    "  subsampling: 32000\n",
    "  r: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a_CDMCi80_DW",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-n NMODEL] [-w WEIGHT] [-t TASK] [-s SCORE]\n",
      "                             [-saf SAF] [-dsdf DSDF]\n",
      "                             model\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/anaconda3/envs/airfrans_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#@title Main script, EDITED to use dSDF data and train GUNet with aSGCN_conv\n",
    "\n",
    "#edited from: https://github.com/Extrality/AirfRANS/blob/main/main.py\n",
    "\n",
    "import argparse, yaml, os, json, glob\n",
    "import torch\n",
    "import train, metrics\n",
    "from dataset import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('model', help = 'The model you want to train, choose between MLP, GraphSAGE, PointNet, GUNet', type = str)\n",
    "parser.add_argument('-n', '--nmodel', help = 'Number of trained models for standard deviation estimation (default: 1)', default = 1, type = int)\n",
    "parser.add_argument('-w', '--weight', help = 'Weight in front of the surface loss (default: 1)', default = 1, type = float)\n",
    "parser.add_argument('-t', '--task', help = 'Task to train on. Choose between \"full\", \"scarce\", \"reynolds\" and \"aoa\" (default: full)', default = 'full', type = str)\n",
    "parser.add_argument('-s', '--score', help = 'If you want to compute the score of the models on the associated test set. (default: 0)', default = 0, type = int)\n",
    "parser.add_argument('-saf','--saf',default = 1, type = int, help = 'Use saf for training')\n",
    "parser.add_argument('-dsdf','--dsdf', default = 1, type=int, help = 'Use dsdf for training')\n",
    "args = parser.parse_args()\n",
    "\n",
    "with open('/home/jessica/Downloads/submission/AirfRANS/Dataset/manifest.json', 'r') as f:\n",
    "    manifest = json.load(f)\n",
    "with open('/home/jessica/Downloads/submission/AirfRANS/Dataset/manifest_dsdf.json', 'r') as f:\n",
    "    manifest_dsdf = json.load(f)\n",
    "with open('/home/jessica/Downloads/submission/AirfRANS/Dataset/manifest_saf.json', 'r') as f:\n",
    "    manifest_saf = json.load(f)\n",
    "\n",
    "\n",
    "manifest_train = manifest[args.task + '_train']\n",
    "test_dataset = manifest[args.task + '_test'] if args.task != 'scarce' else manifest['full_test']\n",
    "n = int(.1*len(manifest_train))\n",
    "train_dataset = manifest_train[:-n]\n",
    "val_dataset = manifest_train[-n:]\n",
    "\n",
    "# if os.path.exists('Dataset/train_dataset'):\n",
    "#     train_dataset = torch.load('Dataset/train_dataset')\n",
    "#     val_dataset = torch.load('Dataset/val_dataset')\n",
    "#     coef_norm = torch.load('Dataset/normalization')\n",
    "# else:\n",
    "train_dataset, coef_norm = Dataset(train_dataset, norm = True, sample = None, \\\n",
    "        use_saf = args.saf,use_dsdf = args.dsdf, manifest_dsdf = manifest_dsdf,manifest_saf = manifest_saf)\n",
    "# torch.save(train_dataset, 'Dataset/train_dataset')\n",
    "# torch.save(coef_norm, 'Dataset/normalization')\n",
    "val_dataset = Dataset(val_dataset, sample = None, coef_norm = coef_norm)\n",
    "# torch.save(val_dataset, 'Dataset/val_dataset')\n",
    "\n",
    "# Cuda\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = 'cuda:1' if use_cuda else 'cpu' #EDITED####\n",
    "if use_cuda:\n",
    "    print('Using: ',device) #print('Using GPU')\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "#with open('params.yaml', 'r') as f: # hyperparameters of the model\n",
    "with open('paramsNEW.yaml', 'r') as f: #EDITED####\n",
    "    hparams = yaml.safe_load(f)[args.model]\n",
    "\n",
    "from models.MLP import MLP\n",
    "models = []\n",
    "for i in range(args.nmodel):\n",
    "    encoder = MLP(hparams['encoder'], batch_norm = False)\n",
    "    decoder = MLP(hparams['decoder'], batch_norm = False)\n",
    "\n",
    "    if args.model == 'GraphSAGE':\n",
    "        from models.GraphSAGE import GraphSAGE\n",
    "        model = GraphSAGE(hparams, encoder, decoder)\n",
    "    \n",
    "    elif args.model == 'PointNet':\n",
    "        from models.PointNet import PointNet\n",
    "        model = PointNet(hparams, encoder, decoder)\n",
    "\n",
    "    elif args.model == 'MLP':\n",
    "        from models.NN import NN\n",
    "        model = NN(hparams, encoder, decoder)\n",
    "\n",
    "    elif args.model == 'GUNet':\n",
    "        from models.GUNet import GUNet\n",
    "        model = GUNet(hparams, encoder, decoder)\n",
    "        \n",
    "    elif args.model == 'GUNetFVGCN':\n",
    "        from models.GUNetFVGCN import GUNetFVGCN\n",
    "        model = GUNetFVGCN(hparams, encoder, decoder)\n",
    "\n",
    "    path = 'metrics/' # path where you want to save log and figures\n",
    "    model = train.main(device, train_dataset, val_dataset, model, hparams, path, \n",
    "                criterion = 'MSE_weighted', val_iter = None, reg = args.weight, name_mod = args.model, val_sample = True,\\\n",
    "                      use_saf = args.saf, use_dsdf = args.dsdf)\n",
    "    models.append(model)\n",
    "torch.save(models, args.model+'_aSGN_SAF_dSDF') #EDITED####\n",
    "\n",
    "if bool(args.score):\n",
    "    s = args.task + '_test' if args.task != 'scarce' else 'full_test'\n",
    "    coefs = metrics.Results_test(device, [models], hparams, coef_norm, n_test = 3, path_in = 'Dataset/', criterion = 'MSE', s = s)\n",
    "    # models can be a stack of the same model (for example MLP) on the task s, if you have another stack of another model (for example GraphSAGE)\n",
    "    # you can put in model argument [models_MLP, models_GraphSAGE] and it will output the results for both models (mean and std) in an ordered array.\n",
    "    np.save('scores/' + args.task + '/true_coefs', coefs[0])\n",
    "    np.save('scores/' + args.task + '/pred_coefs_mean', coefs[1])\n",
    "    np.save('scores/' + args.task + '/pred_coefs_std', coefs[2])\n",
    "    for n, file in enumerate(coefs[3]):\n",
    "        np.save('scores/' + args.task + '/true_surf_coefs_' + str(n), file)\n",
    "    for n, file in enumerate(coefs[4]):\n",
    "        np.save('scores/' + args.task + '/surf_coefs_' + str(n), file)\n",
    "    np.save('scores/' + args.task + '/true_bls', coefs[5])\n",
    "    np.save('scores/' + args.task + '/bls', coefs[6])\n",
    "    for aero in glob.glob('airFoil2D*'):\n",
    "        os.rename(aero, 'scores/' + args.task + '/' + aero)\n",
    "    os.rename('score.json', 'scores/' + args.task + '/score.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=0, out_features=5, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/anaconda3/envs/airfrans_env/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "lin_in = torch.nn.Linear(0, 5)\n",
    "print(lin_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0EXjrl0e3x5m"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (127919933.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 25\u001b[0;36m\u001b[0m\n\u001b[0;31m    python mainNEWNS.py GNetFVnewGraphSAGE_FV_SAF_dSDF -t full -n 1 -s 1 -p half -cuda cuda:0\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#RUN THIS:\n",
    "\n",
    "#cd /home/jessica/Downloads/submission/AirfRANS/\n",
    "\n",
    "\"\"\"\n",
    "# python mainNEW.py GUNet -t scarce -n 1 -s 1 -p half -cuda cuda:2\n",
    "# python mainNEW.py GUNetSGraphSAGE_SAF_dSDF -t scarce -n 1 -s 1 -p half -cuda cuda:2\n",
    "# python mainNEW_wip.py GNetFVGraphSAGE_FVnoS_SAF_dSDF -t scarce -n 1 -s 1 -cuda cuda:2\n",
    "\n",
    "# python mainNEWNS.py GNetFVnewGraphSAGE_FV_SAF_dSDF -t scarce -n 1 -s 1 -p half -cuda cuda:2\n",
    "# python mainNEWNS.py GNetFVnewGraphSAGE_FV_SAF_dSDF_NS -t scarce -n 1 -s 1 -p half -ns 1 -cuda cuda:2\n",
    "\n",
    "# python inference.py -t scarce -M GNetFVGraphSAGE_FVnoS_SAF_dSDF -m GNetFVGraphSAGE_FVnoS_SAF_dSDF -saf 1 -dsdf 1\n",
    "# python inference.py -t scarce -M GUNetSGraphSAGE_SAF_dSDF -m GUNetSGraphSAGE_SAF_dSDF -saf 1 -dsdf 1\n",
    "# python inference.py -t scarce -M GNetFVnewGraphSAGE_FV_SAF_dSDF -m GNetFVnewGraphSAGE_FV_SAF_dSDF -saf 1 -dsdf 1\n",
    "\"\"\"\n",
    "\n",
    "# 38556\n",
    "# python mainNEW_wip.py GUNetGCN -t scarce -n 1 -s 1 -p half -cuda cuda:0\n",
    "# 39132\n",
    "# python mainNEW_wip.py GUNetGCN_SAF_dSDF -t scarce -n 1 -s 1 -p half -cuda cuda:1\n",
    "# 105201\n",
    "# python mainNEWNS.py GNetFVnewGCN_FVnew_SAF_dSDF -t scarce -n 1 -s 1 -p half -cuda cuda:2\n",
    "\n",
    "# 160465\n",
    "# python mainNEWNS.py GNetFVnewGraphSAGE_FV_SAF_dSDF -t full -n 1 -s 1 -p half -cuda cuda:0\n",
    "\n",
    "#python inference.py -t full -M GNetFVnewGraphSAGE_FV_SAF_dSDF -m Effectiveness_models/GNetFVnewGraphSAGE_FV_SAF_dSDFhalf_fvnew_0.25data_final\n",
    "\n",
    "\n",
    "###### REBUT #####\n",
    "# python mainNEW.py GUNet -t scarce -n 1 -s 1 -p half -cuda cuda:1\n",
    "# python mainNEWNS.py GNetFVnewGraphSAGE_FV_SAF_dSDF -t scarce -n 1 -s 1 -p half -cuda cuda:2\n",
    "\n",
    "# python inference.py -t scarce -M GUNet -m GUNethalf_w1_final\n",
    "# python inference.py -t scarce -M GNetFVnewGraphSAGE_FV_SAF_dSDF -m GNetFVnewGraphSAGE_FV_SAF_dSDF_w1half_final\n",
    "# python inference.py -t scarce -M GNetFVnewGraphSAGE_FV_SAF_dSDF_w2 -m GNetFVnewGraphSAGE_FV_SAF_dSDF_w2half_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 180/180 [01:55<00:00,  1.56it/s]\n",
      "100%|| 20/20 [00:12<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/jessica/Downloads/submission/AirfRANS/Dataset/manifest.json', 'r') as f:\n",
    "    manifest = json.load(f)\n",
    "with open('/home/jessica/Downloads/submission/AirfRANS/Dataset/manifest_dsdf.json', 'r') as f:\n",
    "    manifest_dsdf = json.load(f)\n",
    "with open('/home/jessica/Downloads/submission/AirfRANS/Dataset/manifest_saf.json', 'r') as f:\n",
    "    manifest_saf = json.load(f)\n",
    "\n",
    "\n",
    "manifest_train = manifest[\"scarce\" + '_train'][:10]\n",
    "test_dataset = manifest[\"scarce\" + '_test'] if \"scarce\" != 'scarce' else manifest['full_test']\n",
    "n = int(.1*len(manifest_train))\n",
    "train_dataset = manifest_train[:-n]\n",
    "val_dataset = manifest_train[-n:]\n",
    "\n",
    "train_dataset, coef_norm = Dataset(train_dataset, norm = True, sample = None, \\\n",
    "        use_saf = 1,use_dsdf = 1, manifest_dsdf = manifest_dsdf,manifest_saf = manifest_saf)\n",
    "val_dataset = Dataset(val_dataset, sample = None, coef_norm = coef_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_parameters :  64232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|      | 1/398 [04:21<28:48:17, 261.20s/it, loss_surf=1.69, train_loss=2.91]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m model_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_parameters : \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28msum\u001b[39m([np\u001b[38;5;241m.\u001b[39mprod(p\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model_parameters]))\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMSE_weighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_mod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFVGCN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                      \u001b[49m\u001b[43muse_saf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_dsdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdSDF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/submission/AirfRANS/train.py:185\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(device, train_dataset, val_dataset, Net, hparams, path, criterion, reg, val_iter, name_mod, val_sample, use_saf, use_dsdf)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# for data in train_dataset_sampled:\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#     assert(data.x.size(0)==data.saf.size(0))\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(train_dataset_sampled)\n\u001b[0;32m--> 185\u001b[0m _, _, loss_surf_var, loss_vol_var, loss_surf, loss_vol \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(train_loader)\n\u001b[1;32m    187\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss_surf \u001b[38;5;241m+\u001b[39m loss_vol\n",
      "File \u001b[0;32m~/Downloads/submission/AirfRANS/train.py:56\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(device, model, train_loader, optimizer, scheduler, criterion, reg)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 56\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m avg_loss_per_var \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_per_var\n",
      "File \u001b[0;32m~/anaconda3/envs/airfrans_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airfrans_env/lib/python3.9/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airfrans_env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airfrans_env/lib/python3.9/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/airfrans_env/lib/python3.9/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airfrans_env/lib/python3.9/site-packages/torch/optim/adam.py:265\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    264\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 265\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[1;32m    268\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda:1' if True else 'cpu'\n",
    "\n",
    "with open('paramsNEW.yaml', 'r') as f: #EDITED####\n",
    "    hparams = yaml.safe_load(f)['FVGCN_SAF_dSDF']\n",
    "    \n",
    "from models.MLP import MLP\n",
    "if (hparams['encoder'] is not False):\n",
    "    encoder = MLP(hparams['encoder'], batch_norm = False)\n",
    "    decoder = MLP(hparams['decoder'], batch_norm = False)\n",
    "else:\n",
    "    encoder, decoder = None, None\n",
    "    \n",
    "path = 'metrics/'\n",
    "    \n",
    "from models.FVGCN import FVGCN\n",
    "model = FVGCN(hparams, encoder, decoder)\n",
    "\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print('model_parameters : ',sum([np.prod(p.size()) for p in model_parameters]))\n",
    "\n",
    "model = train.main(device, train_dataset, val_dataset, model, hparams, path, \n",
    "                criterion = 'MSE_weighted', val_iter = None, reg = 1, name_mod = 'FVGCN', val_sample = True,\\\n",
    "                      use_saf = hparams['SAF'], use_dsdf = hparams['dSDF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      3\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m batch \u001b[38;5;241m=\u001b[39m test_dataset[i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "model = torch.load()\n",
    "\n",
    "i=0\n",
    "batch = test_dataset[i]\n",
    "out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMCSgxRiwzjr6VxSmH632T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
